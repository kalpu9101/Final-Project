# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-C9odkMSi5BiLyoN5dX0olvZQ2fBUI7x
"""

data_path = "data/selected_candidates.csv"
model_path = "models/rf_best_model.joblib"

import pandas as pd
import numpy as np
import streamlit as st
import statsmodels as sm
import seaborn as sns

df = pd.read_csv('/content/drive/MyDrive/Dean Dilemma.csv')

df.describe()

pd.set_option('display.float_format', '{:.3f}'.format)
df.describe()

df.set_index("SlNo", inplace=True)

df.info()

df_base = df.copy()
df_base.drop(['Board_SSC','Board_CBSE','Board_ICSE','Board_HSC','Entrance_Test','Placement_B','Gender-B','S-TEST','Placement','S-TEST*SCORE','Percentile_ET'], axis=1, inplace=True)

pd.reset_option('display.float_format')
df_base.quantile(np.arange(0.0,1.1,0.1),numeric_only=True)

df_base.skew(numeric_only=True)

from scipy.stats import shapiro
import numpy as np

for col in df_base.select_dtypes(include=np.number).columns:
    print(f"{col} p-value: {shapiro(df_base[col])[1]:.16f}")

thirdq, firstq = df_base.quantile(0.75, numeric_only=True), df_base.quantile(0.25, numeric_only=True)
IQR = 1.5 * (thirdq - firstq)
outlierhigh = thirdq + IQR
outlierlow = firstq - IQR

print(outlierlow, outlierhigh, sep=" <--> ")

numeric_cols = df_base.select_dtypes(include=np.number).columns
iqr_flags = pd.DataFrame(index=df_base.index)

for col in numeric_cols:
    q1, q3 = df_base[col].quantile([0.25, 0.75])
    iqr = q3 - q1
    low, high = q1 - 1.5*iqr, q3 + 1.5*iqr
    iqr_flags[col] = (df_base[col] < low) | (df_base[col] > high)

contamination = (iqr_flags.any(axis=1)).mean()
print("Contamination:", contamination*100, "%")

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
import numpy as np

# --- Select numeric features for anomaly detection ---
anomaly_inputs = df_base.select_dtypes(include=np.number).columns.tolist()

# Drop rows with missing values in these columns (using anomaly_inputs as subset)
df_base = df_base.dropna(subset=anomaly_inputs)

# --- Build Isolation Forest Model ---
model_IF = IsolationForest(contamination=0.15, random_state=42)
model_IF.fit(df_base[anomaly_inputs])

# Add anomaly scores and labels
df_base['anomaly_scores'] = model_IF.decision_function(df_base[anomaly_inputs])
df_base['anomaly'] = model_IF.predict(df_base[anomaly_inputs])   # -1 = outlier, 1 = inlier

# --- Pairplot Before Cleaning ---
palette_before = {1: "blue", -1: "red"}
sns.pairplot(df_base, vars=anomaly_inputs, hue='anomaly', palette=palette_before)
plt.suptitle("Dean Dilemma Data Before Cleaning (Red = Outliers)", y=1.02)
plt.show()

# --- Remove Outliers ---
df_clean = df_base[df_base['anomaly'] == 1].copy()  # Keep only inliers
df_clean.drop(['anomaly_scores', 'anomaly'], axis=1, inplace=True)

# --- Pairplot After Cleaning ---
# Filter anomaly_inputs to only include columns present in df_clean for the second pairplot
anomaly_inputs_for_clean_plot = [col for col in anomaly_inputs if col in df_clean.columns]
sns.pairplot(df_clean, vars=anomaly_inputs_for_clean_plot)  # No palette needed since no hue
plt.suptitle("Dean Dilemma Data After Cleaning (Outliers Removed)", y=1.02)
plt.show()

# --- Save Clean Data ---
df_clean.to_csv("dean_dilemma_clean.csv", index=False)
print(f"Clean dataset saved. Original rows: {len(df)}, Clean rows: {len(df_clean)}")

removed_cols = [c for c in df.columns if c not in df_clean.columns]
print("Columns to re-add:", removed_cols)

# re-add removed columns for the rows that remain (index-aligned)
# keep only rows that are in df_clean
df_model = df_clean.join(df[removed_cols], how='left')

# check
print("Restored shape:", df_model.shape)
print("Any NaNs in the re-added columns?", df_model[removed_cols].isnull().sum())

drop_cols = [
    'Gender',         # duplicate of Gender-B
    'Placement',      # text version of Placement_B (leaks target)
    'Salary',         # leakage for placement prediction
    'Board_SSC',      # overlapping board info (keep HSC board only)
    'Board_CBSE',     # dummy/overlap columns
    'Board_ICSE',     # dummy/overlap columns
    'S-TEST*SCORE',
    'S-TEST',
    'Entrance_Test',
]

# Separate features (X) and target (y)
y = df['Placement_B']

# Create X by dropping the target and other specified columns
# Make sure to drop 'Placement_B' from X after assigning it to y
cols_to_drop_from_X = drop_cols + ['Placement_B']
X = df.drop(columns=cols_to_drop_from_X, errors='ignore')

# Remove rows where the target variable 'y' is NaN, and align 'X' accordingly
# This is crucial for models that don't handle NaN in the target.
valid_indices = y.dropna().index
X = X.loc[valid_indices]
y = y.loc[valid_indices]

cat_cols = X.select_dtypes(include=['object']).columns.tolist()
num_cols = X.select_dtypes(include=['number']).columns.tolist()

print("Categorical:", cat_cols)
print("Numeric:", num_cols)

all_cols = cat_cols + num_cols
X = df_model[all_cols]
Y = df_model['Placement_B']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    Y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV

random_grid = {
    'n_estimators': [int(x) for x in np.linspace(start = 20, stop = 200, num = 10)],
    'max_features': ['sqrt'], # Changed 'auto' to 'sqrt'
    'max_depth': list(range(1,11)),
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

rf_cv = RandomizedSearchCV(estimator = RandomForestClassifier(),
                           param_distributions = random_grid,
                           n_iter = 100,
                           cv = 3,
                           verbose=2,
                           random_state=42,
                           n_jobs = -1)

# Ensure X_train_encoded is used, as prepared in the previous cell
rf_cv.fit(X_train_encoded, y_train)
rf_cv.best_params_

from sklearn.ensemble import RandomForestClassifier
import pandas as pd
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Identify categorical columns
cat_cols = X_train.select_dtypes(include='object').columns

# Apply one-hot encoding to training and testing sets
X_train_encoded = pd.get_dummies(X_train, columns=cat_cols, drop_first=True)
X_test_encoded = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)

# Align columns - crucial for consistency if categories differ between train and test
train_cols = set(X_train_encoded.columns)
test_cols = set(X_test_encoded.columns)

missing_in_test = list(train_cols - test_cols)
for col in missing_in_test:
    X_test_encoded[col] = 0

missing_in_train = list(test_cols - train_cols)
for col in missing_in_train:
    X_train_encoded[col] = 0

# Ensure the order of columns is the same
X_test_encoded = X_test_encoded[X_train_encoded.columns]

# Use the best parameters found by RandomizedSearchCV
best_params = rf_cv.best_params_

rf_clf_final = RandomForestClassifier(random_state=42, oob_score=True, **best_params)
rf_clf_final.fit(X_train_encoded, y_train)

y_pred_final = rf_clf_final.predict(X_test_encoded)

print("OOB Score:", rf_clf_final.oob_score_)
print("\nAccuracy on Test Set:", accuracy_score(y_test, y_pred_final))
print("\nClassification Report:\n", classification_report(y_test, y_pred_final))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_final))

from sklearn.metrics import ConfusionMatrixDisplay, classification_report

ConfusionMatrixDisplay.from_estimator(rf_clf_final, X_test_encoded, y_test)

importances = rf_clf_final.feature_importances_

feature_importance_df = pd.DataFrame({
    'Feature': X_train_encoded.columns,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print(feature_importance_df.head(10))

from sklearn.feature_selection import SelectKBest, mutual_info_classif

# Use X_train_encoded and y_train for feature selection
selector = SelectKBest(score_func=mutual_info_classif, k=min(10, X_train_encoded.shape[1]))
selector.fit(X_train_encoded, y_train)
scores = pd.Series(selector.scores_, index=X_train_encoded.columns).sort_values(ascending=False).head(15)
print("SelectKBest (mutual info) top features:\n", scores)

import joblib
import pandas as pd

# The actual trained model is rf_clf_final
joblib.dump(rf_clf_final, "rf_best_model.joblib")
print("Saved model to rf_best_model.joblib")

# -------------------------
# After saving: how to load & use the saved pipeline
# -------------------------
# 1) Load the saved pipeline (contains preprocessor + classifier)
model = joblib.load("rf_best_model.joblib")

# 2) Prepare new student data (example)
# IMPORTANT: The new student data MUST have the same columns
# and be preprocessed (e.g., one-hot encoded) in the same way
# as the training data (X_train_encoded).

# Let's create a more complete 'new_students_raw' DataFrame that includes
# all original columns used to create X_train, even if with dummy values
# for categorical features not explicitly in the example. This ensures proper encoding.
new_students_raw = pd.DataFrame([
    {"Gender":"M", "Percent_MBA":76, "Percent_SSC":88, "Percent_HSC":85, "Percent_Degree":72,
     "Percentile_ET":68, "Experience_Yrs":1, "Specialization_MBA":"Marketing & Finance",
     "Marks_Communication":82, "Marks_BOCA":79, "Marks_Projectwork":88,
     "Board_HSC": "Others", "Stream_HSC": "Science", "Course_Degree": "Engineering", "Gender-B": 0, "Degree_Engg": 1},
    {"Gender":"F", "Percent_MBA":62, "Percent_SSC":75, "Percent_HSC":70, "Percent_Degree":65,
     "Percentile_ET":55, "Experience_Yrs":0, "Specialization_MBA":"Marketing & HR",
     "Marks_Communication":60, "Marks_BOCA":58, "Marks_Projectwork":64,
     "Board_HSC": "ISC", "Stream_HSC": "Commerce", "Course_Degree": "Commerce", "Gender-B": 1, "Degree_Engg": 0}
])

# Get categorical columns that were used during training
cat_cols_for_new_data = X_train.select_dtypes(include='object').columns.tolist()

# Apply one-hot encoding to new student data
new_students_encoded = pd.get_dummies(new_students_raw, columns=cat_cols_for_new_data, drop_first=True)

# Align columns - crucial for consistency if categories differ
# Add missing columns to new_students_encoded with 0
missing_in_new = list(set(X_train_encoded.columns) - set(new_students_encoded.columns))
for col in missing_in_new:
    new_students_encoded[col] = 0

# Ensure the order of columns is the same as X_train_encoded
new_students_encoded = new_students_encoded[X_train_encoded.columns]

# 3) Predict placement probability and label
probs = model.predict_proba(new_students_encoded)[:,1]   # probability of being placed
preds = model.predict(new_students_encoded)              # 0/1 label

new_students_raw["placed_prob"] = probs
new_students_raw["placed_pred"] = preds
print(new_students_raw[["placed_prob", "placed_pred"]])

# 4) Use a threshold to select students for placement pipeline (e.g., prob >= 0.7)
selected = new_students_raw[new_students_raw["placed_prob"] >= 0.7]
print("Selected candidates (prob >= 0.7):")
print(selected)

# 5) Export selected to CSV
selected.to_csv("selected_candidates.csv", index=False)
print("Exported selected_candidates.csv")

from google.colab import files
files.download("selected_candidates.csv")
files.download("rf_best_model.joblib")


import streamlit as st
import joblib
import json
from pathlib import Path

BASE = Path(__file__).parent  # if using a single-file Streamlit app

@st.cache_resource
def load_model(model_path=BASE/"rf_model.pkl", cols_path=BASE/"feature_columns.json"):
    # 1) make sure files exist
    if not model_path.exists():
        raise FileNotFoundError(f"Model file not found: {model_path}")
    if not cols_path.exists():
        raise FileNotFoundError(f"Feature columns file not found: {cols_path}")

    # 2) load model
    model = joblib.load(model_path)

    # 3) load feature columns
    with open(cols_path, "r", encoding="utf-8") as f:
        feature_cols = json.load(f)

    return model, feature_cols

try:
    model, feature_cols = load_model()
except Exception as e:
    st.error(f"Failed to load model or feature list: {e}")
    st.stop()
